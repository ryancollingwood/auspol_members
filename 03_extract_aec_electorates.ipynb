{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file 03_extract_aec_electorates already exists.\n"
     ]
    }
   ],
   "source": [
    "! mkdir 03_extract_aec_electorates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"03_extract_aec_electorates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEC Localitites\n",
    "\n",
    "The Australian Electoral Commission website has a page where you can search for localities by postcode or electorate  - https://electorate.aec.gov.au/LocalitySearchResults.aspx\n",
    "\n",
    "Given there isn't an API we will scrape these pages for every electorate, that is present in the in the representives data as exported from the [theyvoteforyou.org.au API](https://theyvoteforyou.org.au/help/data#people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 88.0.4324\n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n",
      "[WDM] - Driver [C:\\Users\\ryanc\\.wdm\\drivers\\chromedriver\\win32\\88.0.4324.96\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Representatives\n",
    "\n",
    "Then get the unique electorates from Representitives we downloaded from theyvoteforyou.org.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "representatives_df = pd.read_csv(\"02_transform_they_vote_for_you/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "electorates = list(representatives_df[\"electorate\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for getting a url for a specific electorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_electorate_url(electorate):\n",
    "    return f'https://electorate.aec.gov.au/LocalitySearchResults.aspx?filter={electorate}&filterby=Electorate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating to Page and Dealing with Pagnation\n",
    "\n",
    "The result for a locality/suburb search may be split over several pages. Furthermore not all of the pages are listed in the results. In order to get all of the results we will need to click through each of the page numbers and then on the `...` link to get additional pages.\n",
    "<img src=\"resources/AEC_Localities.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above example, when we scrape the web page we'll have a DataFrame with page links available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4    5\n",
       "0  1  2  3  4  5  ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example pagination dataframe to demonstrate `get_max_pages` function\n",
    "\n",
    "example_pagination_df = pd.DataFrame([\n",
    "    {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: \"...\"}\n",
    "])\n",
    "\n",
    "example_pagination_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to determine, is not only how many pages there. But we need to also determine if their are more page numbers that aren't currently being displayed - denoted by the `...` text being present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_pages(df):\n",
    "    # get the maximum number of pages    \n",
    "    pages_df = df.T\n",
    "    pages_df.columns = [\"pages\"]\n",
    "    \n",
    "    # by converting to number and coercing non-numeric values\n",
    "    # and if there is a non-numeric value we will have a \n",
    "    # `null` value which we can detect as confimration of\n",
    "    # additional pages\n",
    "    pages_df[\"pages\"] = pd.to_numeric(pages_df[\"pages\"], errors =\"coerce\")\n",
    "    \n",
    "    more_pages = any(pages_df[\"pages\"].isnull())\n",
    "    \n",
    "    try:\n",
    "        max_pages = pages_df[\"pages\"].max()\n",
    "    except:\n",
    "        max_pages = None\n",
    "    \n",
    "    # return the known max number of pages\n",
    "    # and if there are more pages to be fetched\n",
    "    return max_pages, more_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per our pagination example we will get back 5 known pages, and confirmation that there are more page numbers to be fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_pages(example_pagination_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Page Data\n",
    "\n",
    "Given the data we're interested in is contained in `<table>` element, we can use the functionality in [pandas.read_html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html)\n",
    "to parse the webpage return any tables as DataFrames.\n",
    "<img src=\"resources/AEC_Localities_html.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(browser, for_page, do_pagination = False):\n",
    "    \n",
    "    if do_pagination:\n",
    "        print(\"\\tpaging to get additional results\")\n",
    "    \n",
    "    print(f\"\\tgetting page {for_page}\")\n",
    "\n",
    "    if do_pagination or for_page > 1:\n",
    "        try:\n",
    "            # first look at how many links we get back because in the case of \n",
    "            # \"...\" we might get two let's assume the the last item \n",
    "            # we get is the one we want\n",
    "            \n",
    "            link_text = str(for_page)\n",
    "            if do_pagination:\n",
    "                link_text = \"...\"\n",
    "            \n",
    "            found_links = browser.links.find_by_text(link_text)\n",
    "            found_links[-1].click()\n",
    "                        \n",
    "            sleep(0.5)\n",
    "            \n",
    "        except ElementDoesNotExist:\n",
    "            return None, None, None\n",
    "\n",
    "    # extract pages from browser html\n",
    "    # given that it is in a table element we\n",
    "    # can read it into a pandas dataframe \n",
    "    # using pd.read_html\n",
    "    tables = pd.read_html(browser.html)\n",
    "\n",
    "    max_pages, more_pages = get_max_pages(tables[1])\n",
    "    \n",
    "    if max_pages == for_page:\n",
    "        more_pages = False    \n",
    "    \n",
    "    # this occurs when we've attempted to paginate by clicking on \"...\"\n",
    "    # but it takes us backwards, so we've likely got all of the pages\n",
    "    if max_pages < for_page:\n",
    "        print(f\"\\tWe have navigated backwards: max_pages: {max_pages} last page: {for_page}\")\n",
    "        return None, None, None        \n",
    "    \n",
    "    return tables[0], max_pages, more_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this bit of code could do with some refatoring as there's a lot of repeated logic. Part of the challenge is we're having to step through multiple sub-pages and finding out if there are more pages due to the \"...\" link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_electorate_data(electorate):\n",
    "    global OUTPUT_DIR\n",
    "    \n",
    "    destination_file = f\"{OUTPUT_DIR}/{electorate.lower()}.csv\"\n",
    "    \n",
    "    if os.path.exists(destination_file):\n",
    "        print(f\"{electorate} - skipping file already exists\")\n",
    "        return\n",
    "    \n",
    "    url = get_electorate_url(electorate)\n",
    "    \n",
    "    print(f\"{electorate} - {url}\")\n",
    "\n",
    "    # list for storing the localities\n",
    "    locality_dfs = list()\n",
    "\n",
    "    # first navigate to page\n",
    "    browser.visit(url)\n",
    "    sleep(1)\n",
    "    \n",
    "    # counter for the current page\n",
    "    current_page = 1\n",
    "    \n",
    "    # the result of this is a list of dataframes\n",
    "    # the first being the locality data (electorate / postcode)\n",
    "    # the second being the number of pages\n",
    "    tables = pd.read_html(browser.html)\n",
    "\n",
    "    # get the page data, the maximum number of pages, and\n",
    "    # if there are more pages\n",
    "    page_df, max_pages, more_pages = get_page_data(browser, current_page)\n",
    "    \n",
    "    print(f\"\\tmax pages: {max_pages} - more pages: {more_pages}\")\n",
    "\n",
    "    # get the first locality data\n",
    "    locality_dfs.append(page_df)\n",
    "    \n",
    "    while current_page != max_pages:\n",
    "        current_page += 1\n",
    "        \n",
    "        page_df, max_pages, more_pages = get_page_data(browser, current_page)\n",
    "        if page_df is None:\n",
    "            print(f\"\\tCouldn't find a link to click {current_page}\")\n",
    "            break\n",
    "        \n",
    "        locality_dfs.append(page_df)\n",
    "        \n",
    "        if current_page == max_pages and more_pages:\n",
    "            # because pagination takes us to the next page\n",
    "            # increment our current_page\n",
    "            current_page += 1\n",
    "            page_df, max_pages, more_pages = get_page_data(browser, current_page, True)\n",
    "            \n",
    "            # in this case we have probably click the \"...\" link to go backwards\n",
    "            if page_df is None:\n",
    "                break\n",
    "            \n",
    "            print(f\"\\tmax pages: {max_pages} - more pages: {more_pages}\")            \n",
    "            locality_dfs.append(page_df)\n",
    "            \n",
    "\n",
    "    electorate_df = pd.concat(locality_dfs)\n",
    "    electorate_df.to_csv(destination_file, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Scraping\n",
    "\n",
    "This will initiate the scraping, it will take some time so go make yourself a cup of something to drink `:)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adelaide - skipping file already exists\n",
      "Aston - skipping file already exists\n",
      "Ballarat - skipping file already exists\n",
      "Banks - skipping file already exists\n",
      "Barker - skipping file already exists\n",
      "Barton - skipping file already exists\n",
      "Bass - skipping file already exists\n",
      "Bean - skipping file already exists\n",
      "Bendigo - skipping file already exists\n",
      "Bennelong - skipping file already exists\n",
      "Berowra - skipping file already exists\n",
      "Blair - skipping file already exists\n",
      "Blaxland - skipping file already exists\n",
      "Bonner - skipping file already exists\n",
      "Boothby - skipping file already exists\n",
      "Bowman - skipping file already exists\n",
      "Braddon - skipping file already exists\n",
      "Bradfield - skipping file already exists\n",
      "Brand - skipping file already exists\n",
      "Brisbane - skipping file already exists\n",
      "Bruce - skipping file already exists\n",
      "Burt - skipping file already exists\n",
      "Calare - skipping file already exists\n",
      "Calwell - skipping file already exists\n",
      "Canberra - skipping file already exists\n",
      "Canning - skipping file already exists\n",
      "Capricornia - skipping file already exists\n",
      "Casey - skipping file already exists\n",
      "Chifley - skipping file already exists\n",
      "Chisholm - skipping file already exists\n",
      "Clark - skipping file already exists\n",
      "Cook - skipping file already exists\n",
      "Cooper - skipping file already exists\n",
      "Corangamite - skipping file already exists\n",
      "Corio - skipping file already exists\n",
      "Cowan - skipping file already exists\n",
      "Cowper - skipping file already exists\n",
      "Cunningham - skipping file already exists\n",
      "Curtin - skipping file already exists\n",
      "Dawson - skipping file already exists\n",
      "Deakin - skipping file already exists\n",
      "Dickson - skipping file already exists\n",
      "Dobell - skipping file already exists\n",
      "Dunkley - skipping file already exists\n",
      "Durack - skipping file already exists\n",
      "Eden-Monaro - skipping file already exists\n",
      "Fadden - skipping file already exists\n",
      "Fairfax - skipping file already exists\n",
      "Farrer - skipping file already exists\n",
      "Fenner - skipping file already exists\n",
      "Fisher - skipping file already exists\n",
      "Flinders - skipping file already exists\n",
      "Flynn - skipping file already exists\n",
      "Forde - skipping file already exists\n",
      "Forrest - skipping file already exists\n",
      "Fowler - skipping file already exists\n",
      "Franklin - skipping file already exists\n",
      "Fraser - skipping file already exists\n",
      "Fremantle - skipping file already exists\n",
      "Gellibrand - skipping file already exists\n",
      "Gilmore - skipping file already exists\n",
      "Gippsland - skipping file already exists\n",
      "Goldstein - skipping file already exists\n",
      "Gorton - skipping file already exists\n",
      "Grayndler - skipping file already exists\n",
      "Greenway - skipping file already exists\n",
      "Grey - skipping file already exists\n",
      "Griffith - skipping file already exists\n",
      "Groom - skipping file already exists\n",
      "Hasluck - skipping file already exists\n",
      "Herbert - skipping file already exists\n",
      "Higgins - skipping file already exists\n",
      "Hindmarsh - skipping file already exists\n",
      "Hinkler - skipping file already exists\n",
      "Holt - skipping file already exists\n",
      "Hotham - skipping file already exists\n",
      "Hughes - skipping file already exists\n",
      "Hume - skipping file already exists\n",
      "Hunter - skipping file already exists\n",
      "Indi - skipping file already exists\n",
      "Isaacs - skipping file already exists\n",
      "Jagajaga - skipping file already exists\n",
      "Kennedy - skipping file already exists\n",
      "Kingsford Smith - skipping file already exists\n",
      "Kingston - skipping file already exists\n",
      "Kooyong - skipping file already exists\n",
      "La Trobe - skipping file already exists\n",
      "Lalor - skipping file already exists\n",
      "Leichhardt - skipping file already exists\n",
      "Lilley - skipping file already exists\n",
      "Lindsay - skipping file already exists\n",
      "Lingiari - skipping file already exists\n",
      "Longman - skipping file already exists\n",
      "Lyne - skipping file already exists\n",
      "Lyons - skipping file already exists\n",
      "Macarthur - skipping file already exists\n",
      "Mackellar - skipping file already exists\n",
      "Macnamara - skipping file already exists\n",
      "Macquarie - skipping file already exists\n",
      "Makin - skipping file already exists\n",
      "Mallee - skipping file already exists\n",
      "Maranoa - skipping file already exists\n",
      "Maribyrnong - skipping file already exists\n",
      "Mayo - skipping file already exists\n",
      "McEwen - skipping file already exists\n",
      "McMahon - skipping file already exists\n",
      "McPherson - skipping file already exists\n",
      "Melbourne - skipping file already exists\n",
      "Menzies - skipping file already exists\n",
      "Mitchell - skipping file already exists\n",
      "Monash - skipping file already exists\n",
      "Moncrieff - skipping file already exists\n",
      "Moore - skipping file already exists\n",
      "Moreton - skipping file already exists\n",
      "New England - skipping file already exists\n",
      "Newcastle - skipping file already exists\n",
      "Nicholls - skipping file already exists\n",
      "North Sydney - skipping file already exists\n",
      "O'Connor - skipping file already exists\n",
      "Oxley - skipping file already exists\n",
      "Page - skipping file already exists\n",
      "Parkes - skipping file already exists\n",
      "Parramatta - skipping file already exists\n",
      "Paterson - skipping file already exists\n",
      "Pearce - skipping file already exists\n",
      "Perth - skipping file already exists\n",
      "Petrie - skipping file already exists\n",
      "Rankin - skipping file already exists\n",
      "Reid - skipping file already exists\n",
      "Richmond - skipping file already exists\n",
      "Riverina - skipping file already exists\n",
      "Robertson - skipping file already exists\n",
      "Ryan - skipping file already exists\n",
      "Scullin - skipping file already exists\n",
      "Shortland - skipping file already exists\n",
      "Solomon - skipping file already exists\n",
      "Spence - skipping file already exists\n",
      "Stirling - skipping file already exists\n",
      "Sturt - skipping file already exists\n",
      "Swan - skipping file already exists\n",
      "Sydney - skipping file already exists\n",
      "Tangney - skipping file already exists\n",
      "Wannon - skipping file already exists\n",
      "Warringah - skipping file already exists\n",
      "Watson - skipping file already exists\n",
      "Wentworth - skipping file already exists\n",
      "Werriwa - skipping file already exists\n",
      "Whitlam - skipping file already exists\n",
      "Wide Bay - skipping file already exists\n",
      "Wills - skipping file already exists\n",
      "Wright - skipping file already exists\n"
     ]
    }
   ],
   "source": [
    "for electorate in sorted(electorates):\n",
    "    save_electorate_data(electorate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python38264bitpythondatacondad75933a419364c33a0623b6a6469479d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
